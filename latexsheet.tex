\documentclass[10pt,landscape]{article}
\usepackage{multicol}
\usepackage{calc}
\usepackage{amssymb}
\usepackage{ifthen}
\usepackage{array}
\usepackage[landscape]{geometry}
\usepackage{hyperref}
\usepackage[fontsize=6pt]{fontsize}
\usepackage{amsmath}
% To make this come out properly in landscape mode, do one of the following
% 1.
%  pdflatex latexsheet.tex
%
% 2.
%  latex latexsheet.tex
%  dvips -P pdf  -t landscape latexsheet.dvi
%  ps2pdf latexsheet.ps


% If you're reading this, be prepared for confusion.  Making this was
% a learning experience for me, and it shows.  Much of the placement
% was hacked in; if you make it better, let me know...


% 2008-04
% Changed page margin code to use the geometry package. Also added code for
% conditional page margins, depending on paper size. Thanks to Uwe Ziegenhagen
% for the suggestions.

% 2006-08
% Made changes based on suggestions from Gene Cooperman. <gene at ccs.neu.edu>


% To Do:
% \listoffigures \listoftables
% \setcounter{secnumdepth}{0}


% This sets page margins to .5 inch if using letter paper, and to 1cm
% if using A4 paper. (This probably isn't strictly necessary.)
% If using another size paper, use default 1cm margins.
\ifthenelse{\lengthtest { \paperwidth = 11in}}
	{ \geometry{top=.5in,left=.5in,right=.5in,bottom=.5in} }
	{\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
	}

% Turn off header and footer
\pagestyle{empty}
 

% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {.2ex}%
                                {.2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother

% Define BibTeX command
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Don't print section numbers
\setcounter{secnumdepth}{0}


\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}


% -----------------------------------------------------------------------

\begin{document}

\raggedright
\footnotesize
\begin{multicols}{3}


% multicol parameters
% These lengths are set only within the two main columns
%\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\section{Data/R}
\subsection{Data Visualization}
\begin{tabular}{@{}ll@{}}
\verb!geom_hist and geom_density!    & distribution of numerical columns \\
\verb!geom_bar!  & number of occurences in a categorical col \\
\verb!geom_boxplot! & shape \& distribution of numerical vars \\
\verb!geom_scatter + geom_line*!  & numerical vs. numerical \\
\verb!geom_bar!  & bar plot for count of categorical vars \\
\verb!geom_hline(yintercept)!  & horizontal line\\
\verb!geom_vline(xintercept)!  & vertical line\\
\verb!geom_abline(slope, intercept)!  & linear function, requires  \\
\verb!geom_segment!  & straight line between (x, y) and (xend, yend) \\
\verb!geom_smooth!  & plots a line/curve of best fit \\
\end{tabular}

*\verb!geom_line! only makes sense with an ordering (e.g. the x-axis is year and observations connect together)

\subsection{Data Manipulation}
\newlength{\MyLen}
\settowidth{\MyLen}{\texttt{letterpaper}/\texttt{a4paper} \ }
\begin{tabular}{@{}p{\the\MyLen}%
                @{}p{\linewidth-\the\MyLen}@{}}
\verb!arrange(asc(col))!  & arranges $col$ by ascending order\\
\verb!arrange(desc(col))!  & arranges $col$ by descending order\\
\verb!relocate(data, col, .before, .after)!  & \hskip6.5em relocates a column relative to its neighbors*\\
\verb!arrange(desc(col))!  & arranges $col$ by descending order\\
\verb!slice(data, pos)!  & indexes rows\\
\verb!bind_rows(df1, df2, ...)!  & \hskip1.5em dfs w/ same columns, concats rows\\
\verb!bind_cols(df1, df2, ...)!  & \hskip1.5em dfs w/ same \# rows, concats cols, renames repeated cols\\
\verb!semi_join(x, y, by)!  & returns rows from x w/ matching val for \verb!by! in y\\
\verb!anti_join(x, y, by)!  & returns rows from x w/o a match in y\\
\verb!full_join(x, y, by)!  & standard outer join\\
\verb!left_join(x, y, by)!  & standard left join, \verb!x! is the left df\\
\verb!right_join(x, y, by)!  & standard right join, \verb!y! is the right df\\
\end{tabular}

*specifying no neighbors moves $col$ to leftmost col, specifyfing both is error \\
Suppose we have the following table \verb!fish_encounters!
\begin{tabular}{c|c|c}
        fish & station & seen \\
        \hline
        4842 & Release & 1 \\
        4842 & I80\_1 & 1 \\
        4842 & Lisbon & 1 \\
        4842 & Rstr & 1 \\
        4842 & Base\_TD & 1 \\
        4842 &  BCE  & 1 \\
        4842 & BCW & 1 \\
        4842 & BCE2 & 1 \\
        4842 & BCW2 & 1 \\
        4842 & MAE & 1 \\  
        4845 & BCE & 0 \\  
\end{tabular} \\
pivot\_wider(fish\_encounters, names\_from = station, values\_from = seen, values\_fill = 0)
\begin{tabular}{@{}c@{}|*{10}{@{}c@{}|}}
        Fish & Release & I80\_1 & Lisbon & Rstr & Base\_TD & BCE & BCW & BCE2 & BCW2 & MAE \\
        \hline
        1 & 4842 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\
        2 & 4843 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\
        3 & 4844 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\
        4 & 4845 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 \\
\end{tabular} \\
Suppose we have the following table \verb!billboard!
\begin{tabular}{*{10}{@{}c@{}|}}
        artist &    track & date.entered &  wk1  & wk2 &  wk3  & wk4 &  wk5 &  wk6 &  wk7 \\
        \hline
 2 Pac     & Baby… & 2000-02-26   &   87  &  82   & 72  &  77 &   87   & 94   & 99 \\
 2Ge+her  &  The … & 2000-09-02 &      91 &   87 &   92 &   NA  &  NA &   NA &   NA \\
 3 Doors D… & Kryp… & 2000-04-08  &  81 &   70 &   68 &   67 &   66   & 57  &  54 \\
 3 Doors D… & Loser & 2000-10-21 &      76 &    76 &   72  &  69  &  67 &   65 &   55 \\
 504 Boyz &  Wobb… & 2000-04-15 &     57  &  34 &   25 &   17 &   17 &   31 &   36 \\
\end{tabular} \\
pivot\_longer(billboard, cols = starts\_with("wk"), names\_to = "week", names\_prefix = "wk", values\_to = "rank", values\_drop\_na = TRUE)
\begin{tabular}{*{5}{@{}c@{}|}}
artist & track & date.entered & week & rank \\
\hline
2 Pac & Baby Don't Cry (Keep... & 2000-02-26 & 1 & 87 \\
2 Pac & Baby Don't Cry (Keep... & 2000-02-26 & 2 & 82 \\
2 Pac & Baby Don't Cry (Keep... & 2000-02-26 & 3 & 72 \\
2 Pac & Baby Don't Cry (Keep... & 2000-02-26 & 4 & 77 \\
2 Pac & Baby Don't Cry (Keep... & 2000-02-26 & 5 & 87 \\
2 Pac & Baby Don't Cry (Keep... & 2000-02-26 & 6 & 94 \\
2 Pac & Baby Don't Cry (Keep... & 2000-02-26 & 7 & 99 \\
2Ge+her & The Hardest Part Of ... & 2000-09-02 & 1 & 91 \\
2Ge+her & The Hardest Part Of ... & 2000-09-02 & 2 & 87 \\
2Ge+her & The Hardest Part Of ... & 2000-09-02 & 3 & 92 \\
\end{tabular}

\subsection{Dates \& Strings}
\settowidth{\MyLen}{\texttt{multicol} }
\begin{tabular}{@{}p{\the\MyLen}%
                @{}p{\linewidth-\the\MyLen}@{}}
\verb!ymd(), dmy(), ...!  & \hskip4.5em converts string to datetime according to order of y-m-d\\
\verb!wdate(date)!  & \hskip4.5em gets the day of the week for a given date\\
\verb!strc(str1, str2, ...)!  & \hskip5.5em concatenates strings/vectors of strings\\
\verb!str_detect(str, pattern)!  & \hskip6.5em TRUE if $\exists$ a substring of \verb!str! that matches \verb!pattern!\\
\verb!str_extract(str, pat, group)!  & \hskip8em finds 1st match in \verb!str! for \verb!pat!, \verb!group! takes matched pattern, returns text matching \verb!group!\\
\verb!str_extract_all(string, pattern)!  & \hskip9.5em returns all matches to \verb!pattern! \\
\verb!str_sub(string, start, end)!  & \hskip7.5em indexes into \verb!string! \\
\verb!str_count(string, pattern)!  & \hskip7em count \# of matches to \verb!pattern! in \verb!string!  \\
\end{tabular}

str\_replace(string, pattern, replacement), str\_replace\_all(string, pattern, replacement) - these exist \\
putting color, fill, alpha, etc. outside of aes(), i.e. typically inside of geom\_x() functions will set it as a constant for the whole graph \\
putting color, fill, alpha, etc. inside of aes() typically implies you have a column in your df (like year) that sets the groups appropriately \\
every geom\_x() function inherits the aes() from ggplot, unless they have their own aes() which overrides the ggplot \\
R always prints dates as YYYY-MM-DD

\subsection{Regex}
\settowidth{\MyLen}{\texttt{.author.text.} }
\begin{tabular}{@{}p{\the\MyLen}%
                @{}p{\linewidth-\the\MyLen}@{}}
\verb!\d! & digits \\
\verb!\s!  & whitespace \\
\verb!\w!   & alphabetic and numeral \\
\verb!^! & matches the start of each line \\
\verb!$! & matches the end of each line \\
\verb!?! & 0 or 1 \\
\verb!+! & 1 or more \\
\verb!*! & 0 or more \\
\verb!{n}! & exactly n \\
\verb!{n, }! & n or more \\
\verb!{n, m}! & between n and m \\

\end{tabular}
Capitalizing any of the above is the complement 

You can also create your own character classes using []:
\begin{tabular}{@{}p{\the\MyLen}%
        @{}p{\linewidth-\the\MyLen}@{}}
\verb![abc]! & matches a, b, or c \\
\verb![a-z]!  & matches every character between a and z \\
\verb![^abc]!   & matches anything except a, b, or c \\
\verb![\^\-]!   & matches \textasciicircum or - \\
\end{tabular}
Parenthesis make groups which can be backreferenced \\
pattern <- "(..)\textbackslash\textbackslash1" \#(..) is some pair of anything, and \\1 takes that same pair  \\
fruit \%>\% str\_subset(pattern) \\
"banana"      "coconut"     "cucumber"    "jujube"    "papaya" "salal berry"
\section{Basic Probability}
\subsection{Probability Theory}
For some random variable $X$, $E(X) = \sum_{x = 0}^{n} x * P(X = x)$. \\
The expected value is just the sum of each outcome multiplied by its porbability. \\
$Var(X) = E((X - \mu) ^ 2)$, $\mu = E(X)$ \\
Again, this is just multiplying the squared difference of the mean from each observation with each observation's respective probability, $sum((x-mu)^2*p)$.

Suppose that the distribution of $X$ is proportional with the function $g(x) = 6 - |x - 5|$. \\
Say that we have outcomes $1, 2, \cdots, 10$, this means $P(X = x) = a(6 - |x - 5|)$. \\
We know that the total number of outcomes and number of current outcomes must be proportional to the function. \\
The way to make the number of outcomes proportional is to find $\sum_{i = 1}^{10}6 - |i - 5|$. \\
To keep the possible values proportional, each probability is $\frac{j}{\sum_{i = 1}^{10}6 - |i - 5|}$, $j \in {g(1), g(2), \cdots, g(10)}$.

\subsection{Binomial Distributions}
Properties of Binomials
\begin{tabular}{@{}p{\the\MyLen}%
        @{}p{\linewidth-\the\MyLen}@{}}
\verb!b! & binary outcomes \\
\verb!i! & independence \\
\verb!n! & fixed sample size \\
\verb!s! & same probability \\
\end{tabular} \\
Binomial Formulas \\
\begin{tabular}{@{}p{\the\MyLen}%
        @{}p{\linewidth-\the\MyLen}@{}}
$\mu = np$ & $\sigma^2 = np(1 - p)$ \\
binom prob & $P(X = k) = \binom{n}{k} {p^k(1 - p)^{n-k}}$ \\
\end{tabular}
R Binomial Functions \\
\begin{tabular}{@{}p{\the\MyLen}%
        @{}p{\linewidth-\the\MyLen}@{}}
\verb!rbinom(n, size, prob)! & \hskip3em random binomial samples \\
\verb!dbinom(x, size, prob)! & \hskip3em density fcn at \verb!x! \\
\verb!qbinom(p, size, prob)! & \hskip3em get the smallest value in the \verb!q!th quantile \\
\verb!pbinom(q, size, prob)! & \hskip3em $P(X<= \verb!q!)$ \\
\verb!pbinom(q, size, prob, lower.tail = T)! & \hskip9.5em $1 - P(X<= \verb!q!) = p(X > \verb!q!)$ \\
\end{tabular}
Note that $\binom{n}{k} = \frac{n!}{k!(n-k)!}$
\subsection{Normal Distributions}
\settowidth{\MyLen}{\texttt{.begin.quotation.}}
R Normal Functions*
\begin{tabular}{@{}p{\the\MyLen}%
                @{}p{\linewidth-\the\MyLen}@{}}
\verb!pnorm(q, size, prob)! & \hskip3em $P(X< \verb!q!)$ \\
\end{tabular}
At any given $x$, $X \sim N(\mu, \sigma)$, $P(X = x) = 0$. \\
The standard normal is $X \sim N(0, 1)$. \\
Any normal has its z-scores as equivalent observations in the standard normal.
In other words, $X \sim N(\mu, \sigma) \implies Z = \frac{X - \mu}{\sigma} \sim N(0, 1)$. \\
*includes \verb!rnorm()!, \verb!qnorm()! which have same functionality as the binom fcns \\
Suppose that $\exists X \sim Binom(n, p)$, with $np(1 - p) \geq 10$ \\
Note the conditions this tests, $p$ can't be too close to 0 or 1 (causes skew), and $n$ must be sufficiently large (reduces variance). \\
We can approximate that binomial with $X \sim N(np, \sqrt{np(1 - p)})$. \\
Recall that this appoximation isn't perfect, the normal has an effect of "cutting off" the binomial distribution. \\
Correct for this with $P(X \leq x + .5)$ wheing finding $P(X \leq x)$, $P(X \geq x - .5)$ when finding $P(X \geq x)$ \\
As a general rule, \\
65\% of data 1 SD from the mean \\
95\% of data 2 SD from the mean \\
99\% of data 3 SD from the mean \\
\section{Inference}

\subsection{Inference on Proportions}
\settowidth{\MyLen}{\texttt{.begin.description.}}
Formulas ($\hat{P}$* is a random estimator for point estimate $\hat{p}$):
\begin{tabular}{@{}p{\the\MyLen}%
                @{}p{\linewidth-\the\MyLen}@{}}
$Var(\hat{P})$        &  $Var(\frac{X}{n}) = \frac{Var(X)}{n}$ \\
$SE(\hat{p})$          &  $\sqrt{Var(\hat{P})}$ \\
CI      & \hskip3em  $\hat{p} \pm z * SE$ \\
$z$      & \hskip3em  $qnorm(1 - \frac{p}{2})$* \\
\end{tabular}
*$\hat{P} \sim N(p, \sqrt{\frac{p(1-p)}{n}})$, this is still random\\
*where $p$ is the desired conf interval \\
Agresti-Coull Method (use $\tilde{p}$ in place of $\hat{p}$)\\
\begin{tabular}{@{}p{\the\MyLen}%
        @{}p{\linewidth-\the\MyLen}@{}}
$\tilde{p}$        &  $\frac{x + 2}{n + 4}$ \\
$SE(\tilde{p})$          &  $\sqrt{\frac{\tilde{p}(1 - \tilde{p})}{n + 4}}$ \\
CI      & \hskip3em  $\tilde{p} \pm z * SE$ \\
$z$      & \hskip3em  $qnorm(1 - \frac{p}{2})$* \\
\end{tabular} \\
*where $p$ is the desired conf interval \\
In theory this is a better estimate, still when $SE$ is too small the $CI$ can be too narrow. \\
Using $\tilde{p}$ moves the estimate closer to $.5$. \\
When $\hat{p}$ is closer to 0 or 1 than $p$, $SE$ tends to be underestimated, and vice versa for $\hat{p}$ closer to $.5$ than $p$. \\
Hypothesis testing - determine if a result we found was due to random chance
\begin{enumerate}
        \item Have a binomial model
        \item State $H_0$ and $H_A$
        \item Choose test statistic 
        \item Find p-value and see if it's under some $\alpha$
\end{enumerate}
Assume $H_0$ is true. Now find probability we observed a certain outcome. \\
Suppose $H_0 | X \sim Binom(n, k)$ and $H_0: k = .5$, $H_A: k \ne .5$. 
We observe $j$ successes and $n$ observations in total. Then $p$ is $2 * qnorm(j, n, .5)$, 
since the probability distribution is symmetric and $\ne$ necessitates a 2-sided test. 
$H_0$ also assumes a binomial distribution w/ chance of success being $.5$. \\
Differnce in Proportions\\
\begin{tabular}{@{}p{\the\MyLen}%
        @{}p{\linewidth-\the\MyLen}@{}}

$\bar{p}$          &  $\frac{x_1 + x_1}{n_1 + n_2}$ \\
$SE(\hat{p_1} -\hat{p_2})$          &  $\sqrt{\frac{\bar{p}(1-\bar{p})}{n_1} + \frac{\bar{p}(1-\bar{p})}{n_2}}$ \\
CI      & \hskip3em  $\tilde{p} \pm z * SE$ \\
$z$      & \hskip3em  $qnorm(1 - \frac{p}{2})$* \\
\end{tabular} \\
*where $p$ is the desired conf interval \\
Agresti-Coffe Method (use $\tilde{p}$ in place of $\hat{p}$)\\
\begin{tabular}{@{}p{\the\MyLen}%
        @{}p{\linewidth-\the\MyLen}@{}}
$\tilde{p}$        &  $\frac{x + 1}{n + 2}$ \\
$SE(\tilde{p_1} - \tilde{p_2})$          &  $\sqrt{Var(\tilde{p_1}) + Var(\tilde{p_2})}$ \\
$Var(\tilde{p_i})$          &  $\frac{\tilde{p_i}*(1 - \tilde{p_i})}{(n_i + 2))}$ \\
CI      & \hskip3em  $\tilde{p} \pm z * SE$ \\
$z$      & \hskip3em  $qnorm(1 - \frac{p}{2})$* \\
\end{tabular} \\
Hypothesis Testing Difference in Proportions - determine if result was due to random chance
\begin{enumerate}
        \item Have 2 binomial models
        \item State $H_0$ and $H_A$
        \item Choose test statistic 
        \item Find p-value and see if it's under some $\alpha$
\end{enumerate}
Say that $H_0: p_1 - p_2 = 0$, $H_A: p_1 \ne p_2$. Say that the number of success is $i_1$ and $i_2$ respectively. \\
Since we test $p_1 - p_2$ the differnces will be normally distributed. \\
Estimate the combined probability as $\bar{p} = \frac{i_1 + i_2}{n_1 + n_2}$. \\
Again, assuming $H_0$ is true calculate $z$. $z = \frac{(\hat{p_1} - \hat{p_2}) - (p_1 - p_2)}{SE}$. THIS $p_1 - p_2$ IS THE $p_1 - p_2$ DEFINED BY $H_0$. \\
 $p$ is the area area under the standard normal, or $2 * P(X > z)$ in this case.

\subsection{Inference on Means}
\settowidth{\MyLen}{\texttt{.pageref.marker..}}
Formulas:
\begin{tabular}{@{}p{\the\MyLen}%
                @{}p{\linewidth-\the\MyLen}@{}}
\verb!CI!   &   $z * SE$  \\
$z$   &   $qt(p, n - 1)$  \\
$SE(\bar{x})$   &   $\frac{\sigma}{\sqrt{n}}$  \\
$T$ & $\frac{\bar{X} - \mu_0}{s / \sqrt{n}}$* \\
$s$ & $\sqrt{\frac{\sum_{i = 1}^{n}(x_i - \bar{x})^2}{n - 1}}$
\end{tabular}
*$\mu_0$ is the value assumed to be $\mu$ under $H_0$. Interpret this as the \# of $SE$
above/below the mean of the null distribution. \\
The $p$ value is the area under the $t$ distribution WRT the $T$ statistic, with $n - 1$ degrees of freedom. \\
\textbf{WHEN FINDING THE CI OF ANY NORMAL/T DISTRIBUTIONS THE CRITICAL VALUE IS SOME QNORM/QT ACCORDING TO THE DESIRED CONFIDENCE LEVEL}. \\

\subsection{Inference on Multiple Means}
Data can be paired or unpaired. 
Paired data is observations that are similar, and we are interested in differences between them. \\
\textbf{For paired data:} \\
Consider a new distribution of the \textbf{differences} in each pair of observations. \\
Hypothesis testing, confidence intervals, etc. are exectly the same as inference on a single mean, just on the difference between means this time.
\textbf{For unpaired data:} \\ 
\textbf{If the variance of the 2 distributions is similar, use the 2-sample:}
\begin{tabular}{@{}c@{}|@{}c@{}}
        \hline
$SE(\bar{X}-\bar{Y})$ & $SE=\sqrt{\frac{\sum(x_i-\bar{x})^2+\sum(y_i-\bar{y})^2}{n_x+n_y-2}}\cdot\sqrt{\frac1{n_x}+\frac1{n_y}}$   \\
Statistic & $t_{obs}=\frac{(\bar{X}-\bar{Y})-(\mu_{X0}-\mu_{Y0})}{SE(\bar{X}-\bar{Y})}$  \\
Degrees of freedom & $\textstyle DF=n_x+n_y-2$   \\
Interval & $(\bar{X}-\bar{Y})\pm t_{crit}*SE$   \\
\end{tabular} \\
\textbf{Welch when variance different:} \\
\begin{tabular}{@{}c@{}|@{}c@{}}
        \hline
$SE(\bar{X}-\bar{Y})$ & $SE=\sqrt{\frac{s_x^2}{n_x}+\frac{s_y^2}{n_y}}$  \\
Statistic & $t_{obs}=\frac{(\bar{X}-\bar{Y})-(\mu_{X0}-\mu_{Y0})}{SE(\bar{X}-\bar{Y})}$  \\
Degrees of freedom & $DF=\frac{(s_x^2/n_x\,+\,s_y^2/n_y)^2}{(s_x^2/n_x)^2/(n_x-1)\,+\,(s_y^2/n_y)^2/(n_y-1)}$  \\
Interval & $(\bar{X}-\bar{Y})\pm t_{crit}*SE$  \\
\end{tabular} \\
Where $\mu_{X0} - \mu_{Y0}$ is the difference in means assumed under $H_0$. \\
$p$ process is same as before, find area under $t$ distribution according to $H_a$ WRT the $T$ statistic.
\subsection{T Distributions}
Recall the $T$ statistic for inference on means. \\
$T = \frac{\bar{X} - \mu_0}{s / \sqrt{n}}$, notice that we use $s$, a point estimate for $\sigma$.
This introduces randomness, so $T$ is not quite normally distributed, so we use $t$ distribution. \\
The standard deviation is $\frac{d}{d-2}$, $d > 2$ where $d$ is degrees of freedom. If $d \in \mathbb{Z}$, round it down. 
In practice, the $t$ distribution converges to the normal as $d$ increases. Still, it resembles a stretched normal. 

\section{Regression}
\subsection{Linear Regression}
$y = \beta_0 + \beta_1x_i + \epsilon_i$ ST the line minimizes the sum squared error. \\
\textbf{Formulas:} \\
\begin{tabular}{@{}c@{}c@{}}
        $r = Corr(x, y)$* & \hskip.5em $\frac{1}{n - 1}\sum_{i = 1}^{n}\frac{x_i-\bar{x}}{s_x}\frac{y_i-\bar{y}}{s_y}$\\
        $\hat{\beta_1}$ & $r * \frac{s_y}{s_x}$ \\        
        $\hat{\beta_0}$ & $\bar{y} - \hat{\beta_1}\bar{x}$ \\        
\end{tabular} \\
        *$r$ assumes $x$ \& $y$ linearly related, measures strength of assumed relationship. \\
        The regression line always goes thru $(\bar{x}, \bar{y})$. \\
        If the residual grpah is curved, $\bar{\epsilon} \ne 0$. If there's fanning out/narrowing in residual plot, $SE(\epsilon)$ is not constant. \\
        \textbf{Confidence intervals for $E(y \mid x^*)$} \\
        This is a confidence interval for the mean $y$ given some $x^*$. 
        For example, if there were infinite observations of $x^*$ we would be $p\%$ confident they fall into a given range (you can also think of this as a confidence interval for points regression line passes thru). \\
        $s_{\hat{y}} = \sqrt{\left((n-2)^{-1} \sum_{i=1}^n(y_i - \hat{y}_i)^2\right)\left(n^{-1} + \frac{(x^* - \overline{x})^2}{\sum_{i = 1}^n (x_i - \overline{x})^2} \right)}$ \\
        \textbf{Prediction intervals for $E(y^* \mid x^*)$} \\
        This is a confidence interval for some random $y^*$. So not only do we account for uncertainty in $\beta_0$ and $\beta_1$, we also have randomness in $\epsilon$. Since, by nature observations have randomness baked in which is encoded by $\epsilon$. \\
        As such, this interval is wider than the confidence interval. \\
        Prediction interval is an interval that 95\% of observations in the dataset will be in \\
        $s_{\hat{y}^*} = \sqrt{\left((n-2)^{-1} \sum_{i=1}^n(y_i - \hat{y}_i)^2\right)\left(1 + n^{-1} + \frac{(x^* - \overline{x})^2}{\sum_{i = 1}^n (x_i - \overline{x})^2} \right)}$. \\
        \subsection{Power Law}
        Sometimes a relationship can only be expressed as $y = C * x^\theta$, this is the power law. \\
        In this case apply a log transform to the data, that way it is normal. Any models fit on this data will predict \textbf{$log_{l}$(y)}, however. \\
        The regression line is trying to predict the mean for each observation. That is, each observation will have a bit of randomness, 
        the regression line wants to capture the mean of all the hypothetical observations. So we say $y_i \sim N(log_{l}(C) + \theta x_i, \sigma)$. \\
        To hypothesis test whether the power law is appropriate we carry out the following hypothesis test (assuming $\epsilon \sim N(0, \sigma)$): \\
        $H_0: \theta = 1$, $H_a: \theta \ne 1$. \\
        \textbf{Hypothesis testing:} \\
        \begin{tabular}{@{}p{\the\MyLen}%
                @{}p{\linewidth-\the\MyLen}@{}}
                $T$ & $\frac{\hat{\theta} - 1}{s_{\hat{\theta}}}$ \\
                $s_{\hat{\theta}}$ & $\sqrt{\frac{\sum_{i = 1}^n \epsilon_i^2 / (n - 2)}{\sum_{i = 1}{n}(x_i - \bar{x})^2}}$ 
        \end{tabular}
        Use $n - 2$ degrees of freedom when doing inference on regression. \\
        This has the same process as inference on a single mean. \\



\end{multicols}
\end{document}
